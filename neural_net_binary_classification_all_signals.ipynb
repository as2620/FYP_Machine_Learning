{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Binary Classification of Stress\n",
    "- There will be two classes: Stress and Rest. This is achieved by combining the S1 and S2 class into one class, Stress\n",
    "- This network will aim to use all the signals extracted from the data collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthi\\AppData\\Local\\Temp\\ipykernel_28524\\1608473029.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['co2', 'voc'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest_coil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabdomen_coil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppg_ir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppg_red\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mco2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Extract the specified columns\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m extracted_columns \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_extract\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Drop any rows with missing data\u001b[39;00m\n\u001b[0;32m     42\u001b[0m extracted_columns \u001b[38;5;241m=\u001b[39m extracted_columns\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['co2', 'voc'] not in index\""
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Neural Network for Binary Classification of Stress\n",
    "# - There will be two classes: Stress and Rest. This is achieved by combining the S1 and S2 class into one class, Stress\n",
    "# - This network will aim to use all the signals extracted from the data collection.\n",
    "# \n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "import heartpy as hp\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# %%\n",
    "# Compilation Options\n",
    "CLASS_WEIGHTS = True\n",
    "\n",
    "# %%\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'filtered_stress_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Specify the columns you want to extract\n",
    "columns_to_extract = ['user_id', 'classification', 'chest_coil', 'abdomen_coil', 'ppg_ir', 'ppg_red', 'gsr', 'co2', 'voc']\n",
    "features = ['chest_coil', 'abdomen_coil', 'ppg_ir', 'ppg_red', 'gsr', 'CO2', 'VOC']\n",
    "\n",
    "# Extract the specified columns\n",
    "extracted_columns = data[columns_to_extract]\n",
    "\n",
    "# Drop any rows with missing data\n",
    "extracted_columns = extracted_columns.dropna()\n",
    "\n",
    "# Check for non-numeric values in each feature and replace them with NaNs\n",
    "for feature in features:\n",
    "    extracted_columns[feature] = pd.to_numeric(extracted_columns[feature], errors='coerce')\n",
    "\n",
    "# Check if there are any NaNs in the dataset after replacing non-numeric values\n",
    "if extracted_columns.isnull().values.any():\n",
    "    print(\"NaNs detected after attempting to convert non-numeric values to float. Please check the data.\")\n",
    "    # Handle NaNs based on your requirements, such as removing rows with NaNs or imputing missing values\n",
    "    # For this example, we will remove rows with NaNs\n",
    "    extracted_columns = extracted_columns.dropna()\n",
    "\n",
    "# Filter data \n",
    "for feature in features:\n",
    "    extracted_columns[feature] = scipy.signal.detrend(extracted_columns[feature])\n",
    "    extracted_columns[feature] = hp.filter_signal(extracted_columns[feature], cutoff=1, sample_rate=20.0, filtertype='lowpass', return_top=False)\n",
    "\n",
    "print(extracted_columns.shape)\n",
    "\n",
    "# Ensure we are using the cleaned data for further processing\n",
    "data = extracted_columns\n",
    "\n",
    "# Combine S1 and S2 classes into one \"Stress\" class\n",
    "data['classification'] = data['classification'].replace({'S1': 'Stress', 'S2': 'Stress'})\n",
    "\n",
    "print(data['classification'].value_counts())\n",
    "print(data['classification'].unique())\n",
    "\n",
    "# %%\n",
    "# Feature Extraction\n",
    "# Extract features and labels\n",
    "X = data[features].values\n",
    "y = data['classification'].values\n",
    "user_ids = data['user_id'].values\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape data into sequences\n",
    "time_step_length = 100  \n",
    "num_samples = len(X_scaled) // time_step_length\n",
    "\n",
    "X_sequences = np.array([X_scaled[i*time_step_length:(i+1)*time_step_length] for i in range(num_samples)])\n",
    "y_sequences = np.array([y[i*time_step_length] for i in range(num_samples)])\n",
    "user_id_sequences = np.array([user_ids[i*time_step_length] for i in range(num_samples)])\n",
    "\n",
    "# %%\n",
    "# Split the data into training and testing sets based on user_id to avoid data leakage\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for train_index, test_index in gkf.split(X_sequences, y_sequences, groups=user_id_sequences):\n",
    "    X_train, X_test = X_sequences[train_index], X_sequences[test_index]\n",
    "    y_train, y_test = y_sequences[train_index], y_sequences[test_index]\n",
    "    break  # Use the first split\n",
    "\n",
    "# Check the shape of the reshaped data\n",
    "print(X_train.shape)  # Should be (num_samples, time_step_length, num_features)\n",
    "print(X_test.shape)\n",
    "\n",
    "# %%\n",
    "# Compute class weights\n",
    "if CLASS_WEIGHTS:\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# %%\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step_length, X_train.shape[2])),\n",
    "    Conv1D(32, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and learning rate scheduler\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for learning rate adjustment and early stopping\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "if CLASS_WEIGHTS:\n",
    "    # Train the model with class weights\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
    "                        class_weight=class_weights_dict, callbacks=[lr_reduction, early_stopping])\n",
    "else:\n",
    "    # Train the model without class weights \n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
    "                    callbacks=[lr_reduction, early_stopping])\n",
    "\n",
    "# %%\n",
    "# Binary Classification\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# %%\n",
    "# Plotting training and validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Evaluate the model on training data to get training predictions\n",
    "y_train_pred = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the model on test data to get test predictions\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Confusion matrix for training data\n",
    "train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "train_cmd = ConfusionMatrixDisplay(confusion_matrix=train_cm, display_labels=label_encoder.classes_)\n",
    "\n",
    "# Confusion matrix for test data\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "test_cmd = ConfusionMatrixDisplay(confusion_matrix=test_cm, display_labels=label_encoder.classes_)\n",
    "\n",
    "# Plot confusion matrices\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_cmd.plot(cmap=plt.cm.Blues, ax=plt.gca())\n",
    "plt.title('Training Confusion Matrix')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "test_cmd.plot(cmap=plt.cm.Blues, ax=plt.gca())\n",
    "plt.title('Test Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
